{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f673b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/finnellingwood/Documents/CS457/01_tokenization/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# For NLI zero-shot\n",
    "try:\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "except ImportError:\n",
    "    AutoModelForSequenceClassification = None\n",
    "    AutoTokenizer = None\n",
    "    pipeline = None\n",
    "\n",
    "# For calibration\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import brier_score_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6473bea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SKILLS': [{'perceptual': [{'name': 'Speaker Identification',\n",
       "     'description': 'Recognizing, counting, or detecting speakers or speech events are present in audio.',\n",
       "     'example': 'How many speakers are there in the audio??'},\n",
       "    {'name': 'Speaker Demographics',\n",
       "     'description': 'Identifying characteristics of speakers like age, sex, or background.',\n",
       "     'example': 'How old is the second speaker?'},\n",
       "    {'name': 'Language Identification',\n",
       "     'description': 'Determining the language, dialect, or accent spoken.',\n",
       "     'example': 'Based how they talk, where are the interviewees from?'},\n",
       "    {'name': 'Lexical and Phrase-Level Recognition',\n",
       "     'description': 'Identifying words and short phrases accurately from speech.',\n",
       "     'example': 'What is the term the speaker uses to describe themselves?'},\n",
       "    {'name': 'Prosody Detection',\n",
       "     'description': 'Recognizing rhythm, stress, intonation, and emphasis in speech.',\n",
       "     'example': 'What word sounds important in the answer to the question?'},\n",
       "    {'name': 'Paralinguistic/Emotion Recognition',\n",
       "     'description': 'Detecting emotions or non-verbal cues from voice.',\n",
       "     'example': 'How confidently does the interviewee answer the question?'},\n",
       "    {'name': 'Speech Activity, Turn-Taking and Overlap Detection',\n",
       "     'description': 'Identifying who speaks when, how turns are exchanged, and detecting overlapping speech.',\n",
       "     'example': 'Who decides to end the conversation?'},\n",
       "    {'name': 'Audio Quality, Artifacts & Channel Characteristics',\n",
       "     'description': 'Recognizing sound quality issues, background noise, or distortions to speech.',\n",
       "     'example': 'Based on how they talk, what illness does the speaker have?'}],\n",
       "   'reasoning': [{'name': 'Social Role and Relationship Inference',\n",
       "     'description': 'Inferring relationships among speakers.',\n",
       "     'example': 'What is the nickname given to the friend of the first speaker'},\n",
       "    {'name': 'Speaker Intent, Pragmatics and Causal Reasoning',\n",
       "     'description': 'Understanding why something was said, implied meanings, and cause-effect relationships.',\n",
       "     'example': 'Why does the first speaker slowly repeat everything the second speaker says'},\n",
       "    {'name': 'Quantitative Reasoning (Counting/Arithmetic Comparison)',\n",
       "     'description': 'Using numbers, counting, and basic math to understand spoken information.',\n",
       "     'example': 'How many times does the speaker use foul language?'},\n",
       "    {'name': 'Temporal and Ordering Reasoning',\n",
       "     'description': 'Understanding sequence, timing, and chronological order of events described.',\n",
       "     'example': 'Does the man speak before the woman in this clip?'},\n",
       "    {'name': 'Logical/Consistency Reasoning',\n",
       "     'description': 'Recognizing logical sequences/inconsistencies within spoken content.',\n",
       "     'example': 'How did the speaker misinterpret the directions given by the GPS?'},\n",
       "    {'name': 'Cross-frontier Entity Linking',\n",
       "     'description': 'Connecting spoken references to external entities or concepts beyond the immediate context.',\n",
       "     'example': 'What does the first speaker do that implies they are the CEO of the company?'},\n",
       "    {'name': 'Ground Truth and World Knowledge Integration',\n",
       "     'description': 'Using general knowledge to interpret and verify spoken content.',\n",
       "     'example': 'What is the capital of the country mentioned by the speaker?'},\n",
       "    {'name': 'Contextual/Causal Scenario Reasoning',\n",
       "     'description': 'Understanding situations or events described, including cause-and-effect relationships within a scenario.',\n",
       "     'example': 'What would happen to the second speaker if the things the first speaker says happen'},\n",
       "    {'name': 'Semantic Abstraction and Summarization',\n",
       "     'description': 'Identifying main ideas, themes, or concise summaries from spoken content.',\n",
       "     'example': 'What is the meaning of the poem read by the speaker?'},\n",
       "    {'name': 'Comparative and Preference-Based Judgments',\n",
       "     'description': 'Evaluating and comparing spoken information, identifying preferences or rankings.',\n",
       "     'example': 'Which of the phrases mentioned by the speaker is the shortest'}]}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"SKILLS\": [\n",
    "    {\n",
    "      \"perceptual\": [\n",
    "        {\n",
    "          \"name\": \"Speaker Identification\",\n",
    "          \"description\": \"Recognizing, counting, or detecting speakers or speech events are present in audio.\",\n",
    "          \"example\": \"How many speakers are there in the audio??\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Speaker Demographics\",\n",
    "          \"description\": \"Identifying characteristics of speakers like age, sex, or background.\",\n",
    "          \"example\": \"How old is the second speaker?\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Language Identification\",\n",
    "          \"description\": \"Determining the language, dialect, or accent spoken.\",\n",
    "          \"example\": \"Based how they talk, where are the interviewees from?\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Lexical and Phrase-Level Recognition\",\n",
    "          \"description\": \"Identifying words and short phrases accurately from speech.\",\n",
    "          \"example\": \"What is the term the speaker uses to describe themselves?\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Prosody Detection\",\n",
    "          \"description\": \"Recognizing rhythm, stress, intonation, and emphasis in speech.\",\n",
    "          \"example\": \"What word sounds important in the answer to the question?\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Paralinguistic/Emotion Recognition\",\n",
    "          \"description\": \"Detecting emotions or non-verbal cues from voice.\",\n",
    "          \"example\": \"How confidently does the interviewee answer the question?\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Speech Activity, Turn-Taking and Overlap Detection\",\n",
    "          \"description\": \"Identifying who speaks when, how turns are exchanged, and detecting overlapping speech.\",\n",
    "          \"example\": \"Who decides to end the conversation?\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Audio Quality, Artifacts & Channel Characteristics\",\n",
    "          \"description\": \"Recognizing sound quality issues, background noise, or distortions to speech.\",\n",
    "          \"example\": \"Based on how they talk, what illness does the speaker have?\"\n",
    "        }\n",
    "      ],\n",
    "      \"reasoning\": [\n",
    "        {\n",
    "          \"name\": \"Social Role and Relationship Inference\",\n",
    "          \"description\": \"Inferring relationships among speakers.\",\n",
    "          \"example\": \"What is the nickname given to the friend of the first speaker\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Speaker Intent, Pragmatics and Causal Reasoning\",\n",
    "          \"description\": \"Understanding why something was said, implied meanings, and cause-effect relationships.\",\n",
    "          \"example\": \"Why does the first speaker slowly repeat everything the second speaker says\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Quantitative Reasoning (Counting/Arithmetic Comparison)\",\n",
    "          \"description\": \"Using numbers, counting, and basic math to understand spoken information.\",\n",
    "          \"example\": \"How many times does the speaker use foul language?\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Temporal and Ordering Reasoning\",\n",
    "          \"description\": \"Understanding sequence, timing, and chronological order of events described.\",\n",
    "          \"example\": \"Does the man speak before the woman in this clip?\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Logical/Consistency Reasoning\",\n",
    "          \"description\": \"Recognizing logical sequences/inconsistencies within spoken content.\",\n",
    "          \"example\": \"How did the speaker misinterpret the directions given by the GPS?\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Cross-frontier Entity Linking\",\n",
    "          \"description\": \"Connecting spoken references to external entities or concepts beyond the immediate context.\",\n",
    "          \"example\": \"What does the first speaker do that implies they are the CEO of the company?\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Ground Truth and World Knowledge Integration\",\n",
    "          \"description\": \"Using general knowledge to interpret and verify spoken content.\",\n",
    "          \"example\": \"What is the capital of the country mentioned by the speaker?\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Contextual/Causal Scenario Reasoning\",\n",
    "          \"description\": \"Understanding situations or events described, including cause-and-effect relationships within a scenario.\",\n",
    "          \"example\": \"What would happen to the second speaker if the things the first speaker says happen\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Semantic Abstraction and Summarization\",\n",
    "          \"description\": \"Identifying main ideas, themes, or concise summaries from spoken content.\",\n",
    "          \"example\": \"What is the meaning of the poem read by the speaker?\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Comparative and Preference-Based Judgments\",\n",
    "          \"description\": \"Evaluating and comparing spoken information, identifying preferences or rankings.\",\n",
    "          \"example\": \"Which of the phrases mentioned by the speaker is the shortest\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31be7e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLITagger:\n",
    "    \"\"\"Use an MNLI model to score (question, skill) pairs.\n",
    "\n",
    "    The hypothesis template: \"The question requires: {skill.description}\"\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"microsoft/deberta-v3-large-mnli\", device: int = -1):\n",
    "        if pipeline is None:\n",
    "            raise ImportError(\"transformers not installed. Please pip install transformers.\")\n",
    "        self.pipe = pipeline(\"text-classification\", model=model_name, tokenizer=model_name, device=device, return_all_scores=True)\n",
    "        # Map label names to a consistent order\n",
    "        self.label_map = {\"ENTAILMENT\": \"yes\", \"CONTRADICTION\": \"no\", \"NEUTRAL\": \"maybe\"}\n",
    "    \n",
    "    def score(self, question: str, skill_desc: str) -> float:\n",
    "        hyp = f\"The question requires: {skill_desc}\"\n",
    "        outputs = self.pipe({\"text\": question, \"text_pair\": hyp})\n",
    "        probs = {o['label'].lower(): o['score'] for o in outputs}\n",
    "        entail = probs.get(\"yes\") or probs.get(\"ENTAILMENT\") or 0.0\n",
    "        return float(entail)\n",
    "    \n",
    "    def tag(self, question: str, skills: List[Dict], threshold: float = 0.5):\n",
    "        results = []\n",
    "        for s in skills:\n",
    "            p = self.score(question, s[\"description\"])\n",
    "            tag = \"YES\" if p >= threshold else \"NO\"\n",
    "            results.append({\"name\": s[\"name\"], \"tag\": tag, \"confidence\": p})\n",
    "        return {\"skills\": results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0785b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_PROMPT_TEMPLATE = (\n",
    "    \"You are a classifier. Follow the JSON schema exactly.\\n\\n\"\n",
    "    \"Question:\\n{question}\\n\\n\"\n",
    "    # \"Question-ID:\\n{ID}\\n\\n\"\n",
    "    \"Skills:\\n{skills_list}\\n\\n\"\n",
    "    \"Instruction:\\nFor each skill, answer with:\\n\"\n",
    "    '- tag: \"YES\" or \"NO\"\\n'\n",
    "    \"- confidence: float between 0 and 1\\n\"\n",
    "    \"- brief_rationale: <= 20 words\\n\\n\"\n",
    "    \"Return ONLY valid JSON of the form:\\n\"\n",
    "    \"{{\\n\"\n",
    "    '  \"skills\": [\\n'\n",
    "    '    {{\"name\": \"...\", \"tag\": \"YES|NO\", \"confidence\": 0.0-1.0, \"brief_rationale\": \"...\"}}\\n'\n",
    "    \"  ]\\n\"\n",
    "    \"}}\\n\"\n",
    ")\n",
    "\n",
    "def build_skills_list(skills: List[Dict]) -> str:\n",
    "    lines = []\n",
    "    for i, s in enumerate(skills, 1):\n",
    "        lines.append(f\"{i}. {s['name']} â€” {s['description']}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def call_llm_zero_shot(question: str, skills: List[Dict]) -> Dict:\n",
    "    prompt = LLM_PROMPT_TEMPLATE.format(question=question, skills_list=build_skills_list(skills))\n",
    "\n",
    "    return {\n",
    "        \"skills\": [\n",
    "            {\"name\": s[\"name\"], \"tag\": random.choice([\"YES\", \"NO\"]), \"confidence\": round(random.uniform(0.4, 0.9), 2), \"brief_rationale\": \"placeholder\"}\n",
    "            for s in skills\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5d77279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_consistency_vote(question: str, skills: List[Dict], infer_fn: Callable[[str, List[Dict]], Dict], k: int = 5):\n",
    "    \"\"\"Run k stochastic passes and aggregate majority vote and mean probabilities.\n",
    "\n",
    "    infer_fn should return dict with structure {\"skills\":[{\"name\":..,\"tag\":..,\"confidence\":..},...]}\n",
    "\n",
    "    \"\"\"\n",
    "    votes = {s['name']: [] for s in skills}\n",
    "    confs = {s['name']: [] for s in skills}\n",
    "    for _ in range(k):\n",
    "        out = infer_fn(question, skills)\n",
    "        for item in out['skills']:\n",
    "            votes[item['name']].append(item['tag'])\n",
    "            confs[item['name']].append(item.get('confidence', 0.5))\n",
    "    results = []\n",
    "    for s in skills:\n",
    "        name = s['name']\n",
    "        yes_count = votes[name].count('YES')\n",
    "        no_count = votes[name].count('NO')\n",
    "        tag = 'YES' if yes_count >= no_count else 'NO'\n",
    "        conf = float(np.mean(confs[name]))\n",
    "        results.append({\"name\": name, \"tag\": tag, \"confidence\": conf, \"votes\": votes[name]})\n",
    "    return {\"skills\": results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73b74896",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemperatureScaler:\n",
    "    \"\"\"Simple temperature scaling for binary probabilities.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.temperature_ = 1.0\n",
    "\n",
    "    def fit(self, probs: np.ndarray, labels: np.ndarray):\n",
    "        X = np.log(probs / (1 - probs)).reshape(-1, 1)\n",
    "        lr = LogisticRegression(solver='lbfgs')\n",
    "        lr.fit(X, labels)\n",
    "        # temperature = 1 / coef\n",
    "        self.temperature_ = 1.0 / lr.coef_[0][0]\n",
    "\n",
    "    def transform(self, probs: np.ndarray) -> np.ndarray:\n",
    "        logits = np.log(probs / (1 - probs))\n",
    "        scaled = 1 / (1 + np.exp(-logits / self.temperature_))\n",
    "        return scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64270f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(results: Dict) -> Dict:\n",
    "    \"\"\"Example consistency rule: if 'Speaker Identification' is NO, then 'Speech Activity...' shouldn't be YES.\n",
    "\n",
    "    Add arbitrary domain rules here.\n",
    "    \"\"\"\n",
    "    rmap = {r['name']: r for r in results['skills']}\n",
    "    if rmap['Speaker Identification']['tag'] == 'NO' and rmap['Speech Activity, Turn-Taking and Overlap Detection']['tag'] == 'YES':\n",
    "        # downgrade to NO with low confidence\n",
    "        rmap['Speech Activity, Turn-Taking and Overlap Detection']['tag'] = 'NO'\n",
    "        rmap['Speech Activity, Turn-Taking and Overlap Detection']['confidence'] = min(\n",
    "            rmap['Speech Activity, Turn-Taking and Overlap Detection']['confidence'], 0.49\n",
    "        )\n",
    "    return {\"skills\": list(rmap.values())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab92176b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SKILLS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m QUESTION = \u001b[33m\"\u001b[39m\u001b[33mIn the recording, who speaks after the host interrupts, and how many different guests are there overall?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m raw_results = call_llm_zero_shot(QUESTION, \u001b[43mSKILLS\u001b[49m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapproved_skill_tags\u001b[39m(\n\u001b[32m      7\u001b[39m     question: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m      8\u001b[39m     skills=SKILLS,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     threshold: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.5\u001b[39m         \n\u001b[32m     12\u001b[39m ):\n\u001b[32m     13\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m    Returns a list of approved skill dicts (tag == 'YES') with name & confidence only.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m    Votes, rationales, etc. are stripped out.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'SKILLS' is not defined"
     ]
    }
   ],
   "source": [
    "QUESTION = \"In the recording, who speaks after the host interrupts, and how many different guests are there overall?\"\n",
    "\n",
    "raw_results = call_llm_zero_shot(QUESTION, SKILLS)\n",
    "\n",
    "\n",
    "def approved_skill_tags(\n",
    "    question: str,\n",
    "    skills=SKILLS,\n",
    "    infer_fn=call_llm_zero_shot,   \n",
    "    k: int = 5,                    \n",
    "    threshold: float = 0.5         \n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a list of approved skill dicts (tag == 'YES') with name & confidence only.\n",
    "    Votes, rationales, etc. are stripped out.\n",
    "    \"\"\"\n",
    "    # Aggregate with self-consistency\n",
    "    agg = self_consistency_vote(question, skills, infer_fn, k=k)\n",
    "\n",
    "\n",
    "    final = sanity_check(agg)\n",
    "\n",
    "    approved = [\n",
    "        {\"name\": s[\"name\"], \"confidence\": s.get(\"confidence\", 0.0)}\n",
    "        for s in final[\"skills\"]\n",
    "        if s[\"tag\"] == \"YES\"\n",
    "    ]\n",
    "    return approved\n",
    "    \n",
    "QUESTION = \"In the recording, who speaks after the host interrupts, and how many different guests are there overall?\"\n",
    "approved = approved_skill_tags(QUESTION)\n",
    "print(approved)\n",
    "\n",
    "agg_results = self_consistency_vote(QUESTION, SKILLS, call_llm_zero_shot, k=5)\n",
    "\n",
    "\n",
    "final_results = sanity_check(agg_results)\n",
    "\n",
    "print(json.dumps(final_results, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9676dd5e-11c8-43ae-8be0-145b706889c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Speaker Identification', 'confidence': 0.6199999999999999}, {'name': 'Language Identification', 'confidence': 0.6040000000000001}, {'name': 'Lexical and Phrase-Level Recognition', 'confidence': 0.5940000000000001}, {'name': 'Prosody Detection', 'confidence': 0.63}, {'name': 'Paralinguistic/Emotion Recognition', 'confidence': 0.688}, {'name': 'Audio Quality, Artifacts & Channel Characteristics', 'confidence': 0.542}, {'name': 'Speaker Intent, Pragmatics and Causal Reasoning', 'confidence': 0.6799999999999999}, {'name': 'Temporal and Ordering Reasoning', 'confidence': 0.7}, {'name': 'Logical/Consistency Reasoning', 'confidence': 0.712}, {'name': 'Cross-frontier Entity Linking', 'confidence': 0.688}]\n"
     ]
    }
   ],
   "source": [
    "# === Minimal cell: run tagging and return ONLY approved skills ===\n",
    "\n",
    "def approved_skill_tags(\n",
    "    question: str,\n",
    "    skills=SKILLS,\n",
    "    infer_fn=call_llm_zero_shot,   # swap with NLI or other backend if desired\n",
    "    k: int = 5,                    # self-consistency passes\n",
    "    threshold: float = 0.5         # (used only if your infer_fn returns raw probs)\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a list of approved skill dicts (tag == 'YES') with name & confidence only.\n",
    "    Votes, rationales, etc. are stripped out.\n",
    "    \"\"\"\n",
    "    # 1) Aggregate with self-consistency\n",
    "    agg = self_consistency_vote(question, skills, infer_fn, k=k)\n",
    "\n",
    "    # 2) Sanity / logic checks\n",
    "    final = sanity_check(agg)\n",
    "\n",
    "    # 3) Keep only YES-tagged skills\n",
    "    approved = [\n",
    "        {\"name\": s[\"name\"], \"confidence\": s.get(\"confidence\", 0.0)}\n",
    "        for s in final[\"skills\"]\n",
    "        if s[\"tag\"] == \"YES\"\n",
    "    ]\n",
    "    return approved\n",
    "\n",
    "# ---- Example usage ----\n",
    "QUESTION = \"Why does the man in this say wait? answer: To open the door\"\n",
    "approved = approved_skill_tags(QUESTION)\n",
    "print(approved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a3b97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approved Perceptual Skills:\n",
      " [{'name': 'Language Identification', 'confidence': 0.62}, {'name': 'Paralinguistic/Emotion Recognition', 'confidence': 0.58}]\n",
      "Approved Reasoning Skills:\n",
      " [{'name': 'Quantitative Reasoning (Counting/Arithmetic Comparison)', 'confidence': 0.772}, {'name': 'Logical/Consistency Reasoning', 'confidence': 0.612}]\n"
     ]
    }
   ],
   "source": [
    "# Cell: Separate tagging for perceptual and reasoning skills\n",
    "\n",
    "def approved_perceptual_tags(\n",
    "    question: str,\n",
    "    perceptual_skills=SKILLS[:8],\n",
    "    infer_fn=call_llm_zero_shot,\n",
    "    k: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a list of approved perceptual skill dicts (tag == 'YES')\n",
    "    with name & confidence only.\n",
    "    \"\"\"\n",
    "    # Aggregate with self-consistency\n",
    "    agg = self_consistency_vote(question, perceptual_skills, infer_fn, k=k)\n",
    "    # Sanity check (domain rules for perceptual tasks)\n",
    "    final = sanity_check(agg)\n",
    "    # Filter YES tags\n",
    "    return [\n",
    "        {\"name\": s[\"name\"], \"confidence\": s.get(\"confidence\", 0.0)}\n",
    "        for s in final[\"skills\"]\n",
    "        if s[\"tag\"] == \"YES\"\n",
    "    ]\n",
    "\n",
    "def approved_reasoning_tags(\n",
    "    question: str,\n",
    "    reasoning_skills=SKILLS[8:],\n",
    "    infer_fn=call_llm_zero_shot,\n",
    "    k: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a list of approved reasoning skill dicts (tag == 'YES')\n",
    "    with name & confidence only.\n",
    "    Independent of perceptual tagging.\n",
    "    \"\"\"\n",
    "    # Aggregate with self-consistency\n",
    "    agg = self_consistency_vote(question, reasoning_skills, infer_fn, k=k)\n",
    "    # No shared sanity rules applied here to keep independence\n",
    "    final = agg\n",
    "    # Filter YES tags\n",
    "    return [\n",
    "        {\"name\": s[\"name\"], \"confidence\": s.get(\"confidence\", 0.0)}\n",
    "        for s in final[\"skills\"]\n",
    "        if s[\"tag\"] == \"YES\"\n",
    "    ]\n",
    "\n",
    "# ---- Example usage ----\n",
    "QUESTION = \"In the recording, who speaks after the host interrupts, and how many different guests are there overall?\"\n",
    "\n",
    "perceptual_results = approved_perceptual_tags(QUESTION)\n",
    "reasoning_results   = approved_reasoning_tags(QUESTION)\n",
    "\n",
    "print(\"Approved Perceptual Skills:\\n\", perceptual_results)\n",
    "print(\"Approved Reasoning Skills:\\n\",   reasoning_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d811dd97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'example_results.json'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utility: export results to JSON file\n",
    "def save_results(results: Dict, path: str = \"results.json\"):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    return path\n",
    "\n",
    "# Example (commented):\n",
    "save_results(final_results, \"example_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c77ca28-e359-4101-a071-cedab3bc2bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
