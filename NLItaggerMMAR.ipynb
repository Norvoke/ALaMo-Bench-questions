{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4737b030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.9.86)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (5.0.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement cross-encoder (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for cross-encoder\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch --index-url https://download.pytorch.org/whl/cu121  # adjust cu version\n",
    "%pip install sentence-transformers cross-encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66fc354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4cb77bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae00b690e5548bc98a877636d2168de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tagging MMAR entries:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMAR skill tagging complete and results saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "# --- Device setup ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load data (adjust filename as needed) ---\n",
    "with open('MMAR-meta.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# --- Define skills ---\n",
    "SKILLS = {\n",
    "    \"perceptual\": [\n",
    "        {\"name\": \"Speaker Identification\", \"description\": \"Recognizing, counting, or detecting speakers or speech events are present in audio.\", \"example\": \"How many speakers are there in the audio?\"},\n",
    "        {\"name\": \"Speaker Demographics\", \"description\": \"Identifying characteristics of speakers like age, sex, or background.\", \"example\": \"How old is the second speaker?\"},\n",
    "        {\"name\": \"Language Identification\", \"description\": \"Determining the language, dialect, or accent spoken.\", \"example\": \"Based how they talk, where are the interviewees from?\"},\n",
    "        {\"name\": \"Lexical and Phrase-Level Recognition\", \"description\": \"Identifying words and short phrases accurately from speech.\", \"example\": \"What is the term the speaker uses to describe themselves?\"},\n",
    "        {\"name\": \"Prosody Detection\", \"description\": \"Recognizing rhythm, stress, intonation, and emphasis in speech.\", \"example\": \"What word sounds important in the answer to the question?\"},\n",
    "        {\"name\": \"Paralinguistic/Emotion Recognition\", \"description\": \"Detecting emotions or non-verbal cues from voice.\", \"example\": \"How confidently does the interviewee answer the question?\"},\n",
    "        {\"name\": \"Speech Activity, Turn-Taking and Overlap Detection\", \"description\": \"Identifying who speaks when, how turns are exchanged, and detecting overlapping speech.\", \"example\": \"Who decides to end the conversation?\"},\n",
    "        {\"name\": \"Audio Quality, Artifacts & Channel Characteristics\", \"description\": \"Recognizing sound quality issues, background noise, or distortions to speech.\", \"example\": \"Based on how they talk, what illness does the speaker have?\"}\n",
    "    ],\n",
    "    \"reasoning\": [\n",
    "        {\"name\": \"Social Role and Relationship Inference\", \"description\": \"Inferring relationships among speakers.\", \"example\": \"What is the nickname given to the friend of the first speaker\"},\n",
    "        {\"name\": \"Speaker Intent, Pragmatics and Causal Reasoning\", \"description\": \"Understanding why something was said, implied meanings, and cause-effect relationships.\", \"example\": \"Why does the first speaker slowly repeat everything the second speaker says\"},\n",
    "        {\"name\": \"Quantitative Reasoning (Counting/Arithmetic Comparison)\", \"description\": \"Using numbers, counting, and basic math to understand spoken information.\", \"example\": \"How many times does the speaker use foul language?\"},\n",
    "        {\"name\": \"Temporal and Ordering Reasoning\", \"description\": \"Understanding sequence, timing, and chronological order of events described.\", \"example\": \"Does the man speak before the woman in this clip?\"},\n",
    "        {\"name\": \"Logical/Consistency Reasoning\", \"description\": \"Recognizing logical sequences/inconsistencies within spoken content.\", \"example\": \"How did the speaker misinterpret the directions given by the GPS?\"},\n",
    "        {\"name\": \"Cross-frontier Entity Linking\", \"description\": \"Connecting spoken references to external entities or concepts beyond the immediate context.\", \"example\": \"What does the first speaker do that implies they are the CEO of the company?\"},\n",
    "        {\"name\": \"Ground Truth and World Knowledge Integration\", \"description\": \"Using general knowledge to interpret and verify spoken content.\", \"example\": \"What is the capital of the country mentioned by the speaker?\"},\n",
    "        {\"name\": \"Contextual/Causal Scenario Reasoning\", \"description\": \"Understanding situations or events described, including cause-and-effect relationships within a scenario.\", \"example\": \"What would happen to the second speaker if the things the first speaker says happen\"},\n",
    "        {\"name\": \"Semantic Abstraction and Summarization\", \"description\": \"Identifying main ideas, themes, or concise summaries from spoken content.\", \"example\": \"What is the meaning of the poem read by the speaker?\"},\n",
    "        {\"name\": \"Comparative and Preference-Based Judgments\", \"description\": \"Evaluating and comparing spoken information, identifying preferences or rankings.\", \"example\": \"Which of the phrases mentioned by the speaker is the shortest\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Initialize models ---\n",
    "nli_model = CrossEncoder('cross-encoder/nli-deberta-base', device=device)\n",
    "embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "# --- Embedding prep for human label mapping ---\n",
    "def skill_repr(skill):\n",
    "    return f\"{skill['name']}: {skill['description']}\"\n",
    "\n",
    "perc_skill_texts = [skill_repr(s) for s in SKILLS[\"perceptual\"]]\n",
    "reas_skill_texts = [skill_repr(s) for s in SKILLS[\"reasoning\"]]\n",
    "perc_embeddings = embed_model.encode(perc_skill_texts, normalize_embeddings=True)\n",
    "reas_embeddings = embed_model.encode(reas_skill_texts, normalize_embeddings=True)\n",
    "perc_names = [s[\"name\"] for s in SKILLS[\"perceptual\"]]\n",
    "reas_names = [s[\"name\"] for s in SKILLS[\"reasoning\"]]\n",
    "\n",
    "def map_human_labels_to_defined(human_labels, category):\n",
    "    if category == \"perceptual\":\n",
    "        target_embeddings = perc_embeddings\n",
    "        target_names = perc_names\n",
    "    else:\n",
    "        target_embeddings = reas_embeddings\n",
    "        target_names = reas_names\n",
    "    mapped = set()\n",
    "    for hl in human_labels or []:\n",
    "        hl_emb = embed_model.encode(hl, normalize_embeddings=True)\n",
    "        sims = np.dot(target_embeddings, hl_emb)\n",
    "        best_idx = int(np.argmax(sims))\n",
    "        mapped.add(target_names[best_idx])\n",
    "    return mapped\n",
    "\n",
    "# --- NLI batching ---\n",
    "def batch_entailment_scores(premises, hypotheses, batch_size=32):\n",
    "    assert len(premises) == len(hypotheses)\n",
    "    pairs = list(zip(premises, hypotheses))\n",
    "    entailment_probs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch = pairs[i: i + batch_size]\n",
    "            logits = nli_model.predict(batch, batch_size=len(batch))\n",
    "            probs = torch.softmax(torch.tensor(logits), dim=-1)\n",
    "            entailment_probs.extend([p[2].item() for p in probs])\n",
    "    return entailment_probs\n",
    "\n",
    "def best_skill_from_cat_label(cat_label, skills_list, category_name):\n",
    "    if not cat_label:\n",
    "        return None\n",
    "    premise = f\"The labeled {category_name} category is '{cat_label}'.\"\n",
    "    hypotheses = []\n",
    "    skill_names = []\n",
    "    for skill in skills_list:\n",
    "        if category_name == \"perceptual\":\n",
    "            hyp = f\"This requires perceptual skill: {skill['name']}. {skill['description']} Example: {skill['example']}\"\n",
    "        else:\n",
    "            hyp = f\"This requires reasoning skill: {skill['name']}. {skill['description']} Example: {skill['example']}\"\n",
    "        hypotheses.append(hyp)\n",
    "        skill_names.append(skill[\"name\"])\n",
    "    premises = [premise] * len(hypotheses)\n",
    "    scores = batch_entailment_scores(premises, hypotheses, batch_size=16)\n",
    "    best_idx = int(np.argmax(scores))\n",
    "    return skill_names[best_idx]\n",
    "\n",
    "# --- Safe string helper ---\n",
    "def safe_str(x):\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    return str(x)\n",
    "\n",
    "# --- Thresholds ---\n",
    "perc_thresh = 0.95\n",
    "reas_thresh = 0.99\n",
    "\n",
    "# --- Process entries ---\n",
    "tagged_data = []\n",
    "\n",
    "for entry in tqdm(data, desc=\"Tagging MMAR entries\"):\n",
    "    qid = entry.get(\"id\")\n",
    "    question = safe_str(entry.get(\"question\", \"\"))\n",
    "    answer = safe_str(entry.get(\"answer\", \"\"))\n",
    "    qa_text = f\"{question} {answer}\".strip()\n",
    "\n",
    "    # No explicit human skill annotations in this format\n",
    "    human_perc = set()\n",
    "    human_reas = set()\n",
    "\n",
    "    perceptual_skills = set()\n",
    "    reasoning_skills = set()\n",
    "\n",
    "    # NLI scoring against defined skill hypotheses\n",
    "    perc_premises = []\n",
    "    perc_hypotheses = []\n",
    "    for skill in SKILLS[\"perceptual\"]:\n",
    "        hypothesis = f\"This requires perceptual skill: {skill['name']}. {skill['description']} Example: {skill['example']}\"\n",
    "        perc_premises.append(qa_text)\n",
    "        perc_hypotheses.append(hypothesis)\n",
    "\n",
    "    reas_premises = []\n",
    "    reas_hypotheses = []\n",
    "    for skill in SKILLS[\"reasoning\"]:\n",
    "        hypothesis = f\"This requires reasoning skill: {skill['name']}. {skill['description']} Example: {skill['example']}\"\n",
    "        reas_premises.append(qa_text)\n",
    "        reas_hypotheses.append(hypothesis)\n",
    "\n",
    "    perc_scores_list = batch_entailment_scores(perc_premises, perc_hypotheses, batch_size=16)\n",
    "    reas_scores_list = batch_entailment_scores(reas_premises, reas_hypotheses, batch_size=16)\n",
    "\n",
    "    perc_scores = list(zip([s[\"name\"] for s in SKILLS[\"perceptual\"]], perc_scores_list))\n",
    "    reas_scores = list(zip([s[\"name\"] for s in SKILLS[\"reasoning\"]], reas_scores_list))\n",
    "\n",
    "    for name, score in perc_scores:\n",
    "        if score > perc_thresh:\n",
    "            perceptual_skills.add(name)\n",
    "    for name, score in reas_scores:\n",
    "        if score > reas_thresh:\n",
    "            reasoning_skills.add(name)\n",
    "\n",
    "    # include (empty) human-mapped\n",
    "    perceptual_skills |= human_perc\n",
    "    reasoning_skills |= human_reas\n",
    "\n",
    "    # derive cat_p / cat_r from category / sub-category per MMAR spec\n",
    "    cat_p = None\n",
    "    cat_r = None\n",
    "    best_from_cat_p = None\n",
    "    best_from_cat_r = None\n",
    "\n",
    "    layer = entry.get(\"category\", \"\")\n",
    "    subcat = entry.get(\"sub-category\") or entry.get(\"sub_category\")  # defensive\n",
    "    if layer == \"Perception Layer\":\n",
    "        cat_p = safe_str(subcat)\n",
    "        inferred = best_skill_from_cat_label(cat_p, SKILLS[\"perceptual\"], \"perceptual\")\n",
    "        if inferred:\n",
    "            perceptual_skills.add(inferred)\n",
    "            best_from_cat_p = inferred\n",
    "    elif layer in (\"Cultural Layer\", \"Semantic Layer\"):\n",
    "        cat_r = safe_str(subcat)\n",
    "        inferred = best_skill_from_cat_label(cat_r, SKILLS[\"reasoning\"], \"reasoning\")\n",
    "        if inferred:\n",
    "            reasoning_skills.add(inferred)\n",
    "            best_from_cat_r = inferred\n",
    "\n",
    "    # Fallback to ensure at least one per category\n",
    "    if not perceptual_skills:\n",
    "        best_perc = max(perc_scores, key=lambda x: x[1])[0]\n",
    "        perceptual_skills.add(best_perc)\n",
    "    if not reasoning_skills:\n",
    "        best_reas = max(reas_scores, key=lambda x: x[1])[0]\n",
    "        reasoning_skills.add(best_reas)\n",
    "\n",
    "    tagged_data.append({\n",
    "        \"id\": qid,\n",
    "        \"question_answer\": qa_text,\n",
    "        \"perceptual_skills\": sorted(list(perceptual_skills)),\n",
    "        \"reasoning_skills\": sorted(list(reasoning_skills)),\n",
    "        \"human_mapped_perceptual\": sorted(list(human_perc)),\n",
    "        \"human_mapped_reasoning\": sorted(list(human_reas)),\n",
    "        \"cat_p\": cat_p,\n",
    "        \"cat_r\": cat_r,\n",
    "        \"best_from_cat_p\": best_from_cat_p,\n",
    "        \"best_from_cat_r\": best_from_cat_r,\n",
    "        \"perceptual_scores\": {name: sc for name, sc in perc_scores},\n",
    "        \"reasoning_scores\": {name: sc for name, sc in reas_scores},\n",
    "    })\n",
    "\n",
    "# --- Save output ---\n",
    "with open('tagged_mmar_output.json', 'w') as f:\n",
    "    json.dump(tagged_data, f, indent=2)\n",
    "\n",
    "print(\"MMAR skill tagging complete and results saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "af3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
