{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4737b030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.9.86)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /mnt/shared/juice/Users/research/miniconda3/envs/af3/lib/python3.10/site-packages (5.0.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement cross-encoder (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for cross-encoder\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch --index-url https://download.pytorch.org/whl/cu121  # adjust cu version\n",
    "%pip install sentence-transformers cross-encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cb77bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0460cfb474e140989ed856ad089f15b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tagging entries:   0%|          | 0/1143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skill tagging complete and results saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    " # assuming the import path; your original used from sentence_transformers import CrossEncoder but typically it's from cross_encoder\n",
    "\n",
    "# --- Device setup ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load data ---\n",
    "with open('speech_all(3).json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# --- Define skills (unchanged) ---\n",
    "SKILLS = {\n",
    "    \"perceptual\": [\n",
    "        {\"name\": \"Speaker Identification\", \"description\": \"Recognizing, counting, or detecting speakers or speech events are present in audio.\", \"example\": \"How many speakers are there in the audio?\"},\n",
    "        {\"name\": \"Speaker Demographics\", \"description\": \"Identifying characteristics of speakers like age, sex, or background.\", \"example\": \"How old is the second speaker?\"},\n",
    "        {\"name\": \"Language Identification\", \"description\": \"Determining the language, dialect, or accent spoken.\", \"example\": \"Based how they talk, where are the interviewees from?\"},\n",
    "        {\"name\": \"Lexical and Phrase-Level Recognition\", \"description\": \"Identifying words and short phrases accurately from speech.\", \"example\": \"What is the term the speaker uses to describe themselves?\"},\n",
    "        {\"name\": \"Prosody Detection\", \"description\": \"Recognizing rhythm, stress, intonation, and emphasis in speech.\", \"example\": \"What word sounds important in the answer to the question?\"},\n",
    "        {\"name\": \"Paralinguistic/Emotion Recognition\", \"description\": \"Detecting emotions or non-verbal cues from voice.\", \"example\": \"How confidently does the interviewee answer the question?\"},\n",
    "        {\"name\": \"Speech Activity, Turn-Taking and Overlap Detection\", \"description\": \"Identifying who speaks when, how turns are exchanged, and detecting overlapping speech.\", \"example\": \"Who decides to end the conversation?\"},\n",
    "        {\"name\": \"Audio Quality, Artifacts & Channel Characteristics\", \"description\": \"Recognizing sound quality issues, background noise, or distortions to speech.\", \"example\": \"Based on how they talk, what illness does the speaker have?\"}\n",
    "    ],\n",
    "    \"reasoning\": [\n",
    "        {\"name\": \"Social Role and Relationship Inference\", \"description\": \"Inferring relationships among speakers.\", \"example\": \"What is the nickname given to the friend of the first speaker\"},\n",
    "        {\"name\": \"Speaker Intent, Pragmatics and Causal Reasoning\", \"description\": \"Understanding why something was said, implied meanings, and cause-effect relationships.\", \"example\": \"Why does the first speaker slowly repeat everything the second speaker says\"},\n",
    "        {\"name\": \"Quantitative Reasoning (Counting/Arithmetic Comparison)\", \"description\": \"Using numbers, counting, and basic math to understand spoken information.\", \"example\": \"How many times does the speaker use foul language?\"},\n",
    "        {\"name\": \"Temporal and Ordering Reasoning\", \"description\": \"Understanding sequence, timing, and chronological order of events described.\", \"example\": \"Does the man speak before the woman in this clip?\"},\n",
    "        {\"name\": \"Logical/Consistency Reasoning\", \"description\": \"Recognizing logical sequences/inconsistencies within spoken content.\", \"example\": \"How did the speaker misinterpret the directions given by the GPS?\"},\n",
    "        {\"name\": \"Cross-frontier Entity Linking\", \"description\": \"Connecting spoken references to external entities or concepts beyond the immediate context.\", \"example\": \"What does the first speaker do that implies they are the CEO of the company?\"},\n",
    "        {\"name\": \"Ground Truth and World Knowledge Integration\", \"description\": \"Using general knowledge to interpret and verify spoken content.\", \"example\": \"What is the capital of the country mentioned by the speaker?\"},\n",
    "        {\"name\": \"Contextual/Causal Scenario Reasoning\", \"description\": \"Understanding situations or events described, including cause-and-effect relationships within a scenario.\", \"example\": \"What would happen to the second speaker if the things the first speaker says happen\"},\n",
    "        {\"name\": \"Semantic Abstraction and Summarization\", \"description\": \"Identifying main ideas, themes, or concise summaries from spoken content.\", \"example\": \"What is the meaning of the poem read by the speaker?\"},\n",
    "        {\"name\": \"Comparative and Preference-Based Judgments\", \"description\": \"Evaluating and comparing spoken information, identifying preferences or rankings.\", \"example\": \"Which of the phrases mentioned by the speaker is the shortest\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Initialize models on GPU ---\n",
    "nli_model = CrossEncoder('cross-encoder/nli-deberta-base', device=device)  # ensure CrossEncoder supports device arg\n",
    "embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "# --- Precompute skill embedding mapping ---\n",
    "def skill_repr(skill):\n",
    "    return f\"{skill['name']}: {skill['description']}\"\n",
    "\n",
    "perc_skill_texts = [skill_repr(s) for s in SKILLS[\"perceptual\"]]\n",
    "reas_skill_texts = [skill_repr(s) for s in SKILLS[\"reasoning\"]]\n",
    "# Normalize embeddings (SentenceTransformer with normalize_embeddings=True in encode call)\n",
    "perc_embeddings = embed_model.encode(perc_skill_texts, normalize_embeddings=True)\n",
    "reas_embeddings = embed_model.encode(reas_skill_texts, normalize_embeddings=True)\n",
    "perc_names = [s[\"name\"] for s in SKILLS[\"perceptual\"]]\n",
    "reas_names = [s[\"name\"] for s in SKILLS[\"reasoning\"]]\n",
    "\n",
    "def map_human_labels_to_defined(human_labels, category):\n",
    "    if category == \"perceptual\":\n",
    "        target_embeddings = perc_embeddings\n",
    "        target_names = perc_names\n",
    "    else:\n",
    "        target_embeddings = reas_embeddings\n",
    "        target_names = reas_names\n",
    "\n",
    "    mapped = set()\n",
    "    for hl in human_labels or []:\n",
    "        hl_emb = embed_model.encode(hl, normalize_embeddings=True)\n",
    "        sims = np.dot(target_embeddings, hl_emb)\n",
    "        best_idx = int(np.argmax(sims))\n",
    "        mapped.add(target_names[best_idx])\n",
    "    return mapped\n",
    "\n",
    "# --- Batched entailment scoring helper ---\n",
    "def batch_entailment_scores(premises, hypotheses, batch_size=32):\n",
    "    \"\"\"\n",
    "    Both inputs are lists of equal length; returns list of entailment probabilities.\n",
    "    Assumes the NLI model outputs logits in order [contradiction, neutral, entailment].\n",
    "    \"\"\"\n",
    "    assert len(premises) == len(hypotheses)\n",
    "    pairs = list(zip(premises, hypotheses))\n",
    "    entailment_probs = []\n",
    "    with torch.no_grad():\n",
    "        # cross-encoder.predict accepts list of tuples\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch = pairs[i : i + batch_size]\n",
    "            logits = nli_model.predict(batch, batch_size=len(batch))  # we control inner batching\n",
    "            probs = torch.softmax(torch.tensor(logits), dim=-1)\n",
    "            # extract entailment index 2\n",
    "            entailment_probs.extend([p[2].item() for p in probs])\n",
    "    return entailment_probs\n",
    "\n",
    "# --- Helper to pick best skill from aux label ---\n",
    "def best_skill_from_cat_label(cat_label, skills_list, category_name):\n",
    "    if not cat_label:\n",
    "        return None\n",
    "    premise = f\"The labeled {category_name} category is '{cat_label}'.\"\n",
    "    hypotheses = []\n",
    "    skill_names = []\n",
    "    for skill in skills_list:\n",
    "        if category_name == \"perceptual\":\n",
    "            hyp = f\"This requires perceptual skill: {skill['name']}. {skill['description']} Example: {skill['example']}\"\n",
    "        else:\n",
    "            hyp = f\"This requires reasoning skill: {skill['name']}. {skill['description']} Example: {skill['example']}\"\n",
    "        hypotheses.append(hyp)\n",
    "        skill_names.append(skill[\"name\"])\n",
    "    premises = [premise] * len(hypotheses)\n",
    "    scores = batch_entailment_scores(premises, hypotheses, batch_size=16)\n",
    "    best_idx = int(np.argmax(scores))\n",
    "    return skill_names[best_idx]\n",
    "\n",
    "# --- Thresholds ---\n",
    "perc_thresh = 0.95\n",
    "reas_thresh = 0.99\n",
    "\n",
    "# --- Main loop with batching per entry ---\n",
    "tagged_data = []\n",
    "\n",
    "for entry in tqdm(data, desc=\"Tagging entries\"):\n",
    "    qid = entry.get(\"id\")\n",
    "    qa_text = entry.get(\"question\", \"\") + \" \" + entry.get(\"answer\", \"\")\n",
    "\n",
    "    # human-labeled mapped to defined skill names\n",
    "    human_perc = map_human_labels_to_defined(entry.get(\"perceptual_skills\", []), \"perceptual\")\n",
    "    human_reas = map_human_labels_to_defined(entry.get(\"reasoning_skills\", []), \"reasoning\")\n",
    "\n",
    "    perceptual_skills = set()\n",
    "    reasoning_skills = set()\n",
    "\n",
    "    # Prepare NLI pairs for defined skills\n",
    "    perc_premises = []\n",
    "    perc_hypotheses = []\n",
    "    for skill in SKILLS[\"perceptual\"]:\n",
    "        hypothesis = f\"This requires perceptual skill: {skill['name']}. {skill['description']} Example: {skill['example']}\"\n",
    "        perc_premises.append(qa_text)\n",
    "        perc_hypotheses.append(hypothesis)\n",
    "\n",
    "    reas_premises = []\n",
    "    reas_hypotheses = []\n",
    "    for skill in SKILLS[\"reasoning\"]:\n",
    "        hypothesis = f\"This requires reasoning skill: {skill['name']}. {skill['description']} Example: {skill['example']}\"\n",
    "        reas_premises.append(qa_text)\n",
    "        reas_hypotheses.append(hypothesis)\n",
    "\n",
    "    # Get scores in batch\n",
    "    perc_scores_list = batch_entailment_scores(perc_premises, perc_hypotheses, batch_size=16)\n",
    "    reas_scores_list = batch_entailment_scores(reas_premises, reas_hypotheses, batch_size=16)\n",
    "\n",
    "    perc_scores = list(zip([s[\"name\"] for s in SKILLS[\"perceptual\"]], perc_scores_list))\n",
    "    reas_scores = list(zip([s[\"name\"] for s in SKILLS[\"reasoning\"]], reas_scores_list))\n",
    "\n",
    "    for name, score in perc_scores:\n",
    "        if score > perc_thresh:\n",
    "            perceptual_skills.add(name)\n",
    "\n",
    "    for name, score in reas_scores:\n",
    "        if score > reas_thresh:\n",
    "            reasoning_skills.add(name)\n",
    "\n",
    "    # include human-mapped ones\n",
    "    perceptual_skills |= human_perc\n",
    "    reasoning_skills |= human_reas\n",
    "\n",
    "    # cat_p / cat_r enrichment from aux\n",
    "    aux = entry.get(\"aux\", {})\n",
    "    cat_p = aux.get(\"cat_p\")\n",
    "    cat_r = aux.get(\"cat_r\")\n",
    "\n",
    "    best_from_cat_p = best_skill_from_cat_label(cat_p, SKILLS[\"perceptual\"], \"perceptual\")\n",
    "    if best_from_cat_p:\n",
    "        perceptual_skills.add(best_from_cat_p)\n",
    "    best_from_cat_r = best_skill_from_cat_label(cat_r, SKILLS[\"reasoning\"], \"reasoning\")\n",
    "    if best_from_cat_r:\n",
    "        reasoning_skills.add(best_from_cat_r)\n",
    "\n",
    "    # fallback: ensure at least one per category\n",
    "    if not perceptual_skills:\n",
    "        best_perc = max(perc_scores, key=lambda x: x[1])[0]\n",
    "        perceptual_skills.add(best_perc)\n",
    "    if not reasoning_skills:\n",
    "        best_reas = max(reas_scores, key=lambda x: x[1])[0]\n",
    "        reasoning_skills.add(best_reas)\n",
    "\n",
    "    tagged_data.append({\n",
    "        \"id\": qid,\n",
    "        \"question_answer\": qa_text,\n",
    "        \"perceptual_skills\": sorted(list(perceptual_skills)),\n",
    "        \"reasoning_skills\": sorted(list(reasoning_skills)),\n",
    "        \"human_mapped_perceptual\": sorted(list(human_perc)),\n",
    "        \"human_mapped_reasoning\": sorted(list(human_reas)),\n",
    "        \"cat_p\": cat_p,\n",
    "        \"cat_r\": cat_r,\n",
    "        \"best_from_cat_p\": best_from_cat_p,\n",
    "        \"best_from_cat_r\": best_from_cat_r,\n",
    "        \"perceptual_scores\": {name: sc for name, sc in perc_scores},\n",
    "        \"reasoning_scores\": {name: sc for name, sc in reas_scores},\n",
    "    })\n",
    "\n",
    "# --- Save output ---\n",
    "with open('tagged_ALL.json', 'w') as f:\n",
    "    json.dump(tagged_data, f, indent=2)\n",
    "\n",
    "print(\"Skill tagging complete and results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6625b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "019c3cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9bc4b77ff04109976cafd5ec0cd4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tagging entries:   0%|          | 0/1143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skill tagging complete and results saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    " # assuming the import path; your original used from sentence_transformers import CrossEncoder but typically it's from cross_encoder\n",
    "\n",
    "# --- Device setup ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load data ---\n",
    "with open('speech_all(3).json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# --- Define skills (unchanged) ---\n",
    "SKILLS = {\n",
    "    \"perceptual\": [\n",
    "        {\"name\": \"Speaker Identification\", \"description\": \"Recognizing, counting, or detecting speakers or speech events are present in audio.\", \"example\": \"How many speakers are there in the audio?\"},\n",
    "        {\"name\": \"Speaker Demographics\", \"description\": \"Identifying characteristics of speakers like age, sex, or background.\", \"example\": \"How old is the second speaker?\"},\n",
    "        {\"name\": \"Language Identification\", \"description\": \"Determining the language, dialect, or accent spoken.\", \"example\": \"Based how they talk, where are the interviewees from?\"},\n",
    "        {\"name\": \"Lexical and Phrase-Level Recognition\", \"description\": \"Identifying words and short phrases accurately from speech.\", \"example\": \"What is the term the speaker uses to describe themselves?\"},\n",
    "        {\"name\": \"Prosody Detection\", \"description\": \"Recognizing rhythm, stress, intonation, and emphasis in speech.\", \"example\": \"What word sounds important in the answer to the question?\"},\n",
    "        {\"name\": \"Paralinguistic/Emotion Recognition\", \"description\": \"Detecting emotions or non-verbal cues from voice.\", \"example\": \"How confidently does the interviewee answer the question?\"},\n",
    "        {\"name\": \"Speech Activity, Turn-Taking and Overlap Detection\", \"description\": \"Identifying who speaks when, how turns are exchanged, and detecting overlapping speech.\", \"example\": \"Who decides to end the conversation?\"},\n",
    "        {\"name\": \"Audio Quality, Artifacts & Channel Characteristics\", \"description\": \"Recognizing sound quality issues, background noise, or distortions to speech.\", \"example\": \"Based on how they talk, what illness does the speaker have?\"}\n",
    "    ],\n",
    "    \"reasoning\": [\n",
    "        {\"name\": \"Social Role and Relationship Inference\", \"description\": \"Inferring relationships among speakers.\", \"example\": \"What is the nickname given to the friend of the first speaker\"},\n",
    "        {\"name\": \"Speaker Intent, Pragmatics and Causal Reasoning\", \"description\": \"Understanding why something was said, implied meanings, and cause-effect relationships.\", \"example\": \"Why does the first speaker slowly repeat everything the second speaker says\"},\n",
    "        {\"name\": \"Quantitative Reasoning (Counting/Arithmetic Comparison)\", \"description\": \"Using numbers, counting, and basic math to understand spoken information.\", \"example\": \"How many times does the speaker use foul language?\"},\n",
    "        {\"name\": \"Temporal and Ordering Reasoning\", \"description\": \"Understanding sequence, timing, and chronological order of events described.\", \"example\": \"Does the man speak before the woman in this clip?\"},\n",
    "        {\"name\": \"Logical/Consistency Reasoning\", \"description\": \"Recognizing logical sequences/inconsistencies within spoken content.\", \"example\": \"How did the speaker misinterpret the directions given by the GPS?\"},\n",
    "        {\"name\": \"Cross-frontier Entity Linking\", \"description\": \"Connecting spoken references to external entities or concepts beyond the immediate context.\", \"example\": \"What does the first speaker do that implies they are the CEO of the company?\"},\n",
    "        {\"name\": \"Ground Truth and World Knowledge Integration\", \"description\": \"Using general knowledge to interpret and verify spoken content.\", \"example\": \"What is the capital of the country mentioned by the speaker?\"},\n",
    "        {\"name\": \"Contextual/Causal Scenario Reasoning\", \"description\": \"Understanding situations or events described, including cause-and-effect relationships within a scenario.\", \"example\": \"What would happen to the second speaker if the things the first speaker says happen\"},\n",
    "        {\"name\": \"Semantic Abstraction and Summarization\", \"description\": \"Identifying main ideas, themes, or concise summaries from spoken content.\", \"example\": \"What is the meaning of the poem read by the speaker?\"},\n",
    "        {\"name\": \"Comparative and Preference-Based Judgments\", \"description\": \"Evaluating and comparing spoken information, identifying preferences or rankings.\", \"example\": \"Which of the phrases mentioned by the speaker is the shortest\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Initialize models on GPU ---\n",
    "nli_model = CrossEncoder('cross-encoder/nli-deberta-base', device=device)  # ensure CrossEncoder supports device arg\n",
    "embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "# --- Precompute skill embedding mapping ---\n",
    "def skill_repr(skill):\n",
    "    return f\"{skill['name']}: {skill['description']}\"\n",
    "\n",
    "perc_skill_texts = [skill_repr(s) for s in SKILLS[\"perceptual\"]]\n",
    "reas_skill_texts = [skill_repr(s) for s in SKILLS[\"reasoning\"]]\n",
    "# Normalize embeddings (SentenceTransformer with normalize_embeddings=True in encode call)\n",
    "perc_embeddings = embed_model.encode(perc_skill_texts, normalize_embeddings=True)\n",
    "reas_embeddings = embed_model.encode(reas_skill_texts, normalize_embeddings=True)\n",
    "perc_names = [s[\"name\"] for s in SKILLS[\"perceptual\"]]\n",
    "reas_names = [s[\"name\"] for s in SKILLS[\"reasoning\"]]\n",
    "\n",
    "def map_human_labels_to_defined(human_labels, category):\n",
    "    if category == \"perceptual\":\n",
    "        target_embeddings = perc_embeddings\n",
    "        target_names = perc_names\n",
    "    else:\n",
    "        target_embeddings = reas_embeddings\n",
    "        target_names = reas_names\n",
    "\n",
    "    mapped = set()\n",
    "    for hl in human_labels or []:\n",
    "        # Replace excluded skill with closest alternative\n",
    "        if hl == \"Paralinguistic/Emotion Recognition\" and category == \"perceptual\":\n",
    "            hl_emb = embed_model.encode(hl, normalize_embeddings=True)\n",
    "            sims = np.dot(target_embeddings, hl_emb)\n",
    "            # Mask out the Paralinguistic/Emotion Recognition index\n",
    "            excluded_idx = target_names.index(\"Paralinguistic/Emotion Recognition\")\n",
    "            sims[excluded_idx] = -np.inf\n",
    "            best_idx = int(np.argmax(sims))\n",
    "            mapped.add(target_names[best_idx])\n",
    "        else:\n",
    "            hl_emb = embed_model.encode(hl, normalize_embeddings=True)\n",
    "            sims = np.dot(target_embeddings, hl_emb)\n",
    "            best_idx = int(np.argmax(sims))\n",
    "            mapped.add(target_names[best_idx])\n",
    "    return mapped\n",
    "\n",
    "\n",
    "# --- Batched entailment scoring helper ---\n",
    "def batch_entailment_scores(premises, hypotheses, batch_size=32):\n",
    "    \"\"\"\n",
    "    Both inputs are lists of equal length; returns list of entailment probabilities.\n",
    "    Assumes the NLI model outputs logits in order [contradiction, neutral, entailment].\n",
    "    \"\"\"\n",
    "    assert len(premises) == len(hypotheses)\n",
    "    pairs = list(zip(premises, hypotheses))\n",
    "    entailment_probs = []\n",
    "    with torch.no_grad():\n",
    "        # cross-encoder.predict accepts list of tuples\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch = pairs[i : i + batch_size]\n",
    "            logits = nli_model.predict(batch, batch_size=len(batch))  # we control inner batching\n",
    "            probs = torch.softmax(torch.tensor(logits), dim=-1)\n",
    "            # extract entailment index 2\n",
    "            entailment_probs.extend([p[2].item() for p in probs])\n",
    "    return entailment_probs\n",
    "\n",
    "# --- Helper to pick best skill from aux label ---\n",
    "def best_skill_from_cat_label(cat_label, skills_list, category_name):\n",
    "    if not cat_label:\n",
    "        return None\n",
    "    premise = f\"The labeled {category_name} category is '{cat_label}'.\"\n",
    "    hypotheses = []\n",
    "    skill_names = []\n",
    "    for skill in skills_list:\n",
    "        if category_name == \"perceptual\":\n",
    "            hyp = f\"This requires perceptual skill: {skill['name']}. {skill['description']} Example: {skill['example']}\"\n",
    "        else:\n",
    "            hyp = f\"This requires reasoning skill: {skill['name']}. {skill['description']} Example: {skill['example']}\"\n",
    "        hypotheses.append(hyp)\n",
    "        skill_names.append(skill[\"name\"])\n",
    "    premises = [premise] * len(hypotheses)\n",
    "    scores = batch_entailment_scores(premises, hypotheses, batch_size=16)\n",
    "    best_idx = int(np.argmax(scores))\n",
    "    return skill_names[best_idx]\n",
    "\n",
    "# --- Thresholds ---\n",
    "perc_thresh = 0.94\n",
    "reas_thresh = 0.99\n",
    "\n",
    "# --- Main loop with batching per entry ---\n",
    "tagged_data = []\n",
    "\n",
    "for entry in tqdm(data, desc=\"Tagging entries\"):\n",
    "    qid = entry.get(\"id\")\n",
    "    qa_text = entry.get(\"question\", \"\") + \" \" + entry.get(\"answer\", \"\")\n",
    "\n",
    "    # human-labeled mapped to defined skill names\n",
    "    human_perc = map_human_labels_to_defined(entry.get(\"perceptual_skills\", []), \"perceptual\")\n",
    "    human_reas = map_human_labels_to_defined(entry.get(\"reasoning_skills\", []), \"reasoning\")\n",
    "\n",
    "    perceptual_skills = set()\n",
    "    reasoning_skills = set()\n",
    "\n",
    "    # Prepare NLI pairs for defined skills\n",
    "    perc_premises = []\n",
    "    perc_hypotheses = []\n",
    "    for skill in SKILLS[\"perceptual\"]:\n",
    "        hypothesis = f\"This requires perceptual skill: {skill['name']}. {skill['description']} Example: {skill['example']}\"\n",
    "        perc_premises.append(qa_text)\n",
    "        perc_hypotheses.append(hypothesis)\n",
    "\n",
    "    reas_premises = []\n",
    "    reas_hypotheses = []\n",
    "    for skill in SKILLS[\"reasoning\"]:\n",
    "        hypothesis = f\"This requires reasoning skill: {skill['name']}. {skill['description']} Example: {skill['example']}\"\n",
    "        reas_premises.append(qa_text)\n",
    "        reas_hypotheses.append(hypothesis)\n",
    "\n",
    "    # Get scores in batch\n",
    "    perc_scores_list = batch_entailment_scores(perc_premises, perc_hypotheses, batch_size=16)\n",
    "    reas_scores_list = batch_entailment_scores(reas_premises, reas_hypotheses, batch_size=16)\n",
    "\n",
    "    perc_scores = list(zip([s[\"name\"] for s in SKILLS[\"perceptual\"]], perc_scores_list))\n",
    "    reas_scores = list(zip([s[\"name\"] for s in SKILLS[\"reasoning\"]], reas_scores_list))\n",
    "\n",
    "    for name, score in perc_scores:\n",
    "        if score > perc_thresh:\n",
    "            perceptual_skills.add(name)\n",
    "\n",
    "    for name, score in reas_scores:\n",
    "        if score > reas_thresh:\n",
    "            reasoning_skills.add(name)\n",
    "\n",
    "    # include human-mapped ones\n",
    "    perceptual_skills |= human_perc\n",
    "    reasoning_skills |= human_reas\n",
    "\n",
    "        # --- cat_p / cat_r enrichment from aux, splitting comma-separated labels ---\n",
    "    aux = entry.get(\"aux\", {})\n",
    "\n",
    "    raw_cat_p = aux.get(\"cat_p\")\n",
    "    raw_cat_r = aux.get(\"cat_r\")\n",
    "\n",
    "    # Split comma-separated if present, trim whitespace, ignore empties\n",
    "    def split_labels(raw):\n",
    "        if not raw:\n",
    "            return []\n",
    "        if isinstance(raw, str):\n",
    "            parts = [p.strip() for p in raw.split(\",\")]\n",
    "            return [p for p in parts if p]\n",
    "        if isinstance(raw, list):\n",
    "            return [str(p).strip() for p in raw if str(p).strip()]\n",
    "        return []\n",
    "\n",
    "    cat_p_labels = split_labels(raw_cat_p)\n",
    "    cat_r_labels = split_labels(raw_cat_r)\n",
    "\n",
    "    best_from_cat_p = None\n",
    "    best_from_cat_r = None\n",
    "\n",
    "    # For each label, pick best matching perceptual/reasoning skill and add\n",
    "    for label in cat_p_labels:\n",
    "        inferred = best_skill_from_cat_label(label, SKILLS[\"perceptual\"], \"perceptual\")\n",
    "        if inferred:\n",
    "            perceptual_skills.add(inferred)\n",
    "            best_from_cat_p = best_from_cat_p or inferred  # keep first for reporting\n",
    "\n",
    "    for label in cat_r_labels:\n",
    "        inferred = best_skill_from_cat_label(label, SKILLS[\"reasoning\"], \"reasoning\")\n",
    "        if inferred:\n",
    "            reasoning_skills.add(inferred)\n",
    "            best_from_cat_r = best_from_cat_r or inferred\n",
    "\n",
    "    # Store the original (list) cat_p/cat_r for output\n",
    "    cat_p = cat_p_labels if cat_p_labels else None\n",
    "    cat_r = cat_r_labels if cat_r_labels else None\n",
    "\n",
    "\n",
    "    # fallback: ensure at least one per category\n",
    "    if not perceptual_skills:\n",
    "        if not perceptual_skills:\n",
    "            sorted_perc = sorted(perc_scores, key=lambda x: x[1], reverse=True)\n",
    "            for skill_name, _ in sorted_perc:\n",
    "                if skill_name != \"Paralinguistic/Emotion Recognition\":\n",
    "                    perceptual_skills.add(skill_name)\n",
    "                    break\n",
    "    if not reasoning_skills:\n",
    "        best_reas = max(reas_scores, key=lambda x: x[1])[0]\n",
    "        reasoning_skills.add(best_reas)\n",
    "\n",
    "    tagged_data.append({\n",
    "        \"id\": qid,\n",
    "        \"question_answer\": qa_text,\n",
    "        \"perceptual_skills\": sorted(list(perceptual_skills)),\n",
    "        \"reasoning_skills\": sorted(list(reasoning_skills)),\n",
    "        \"human_mapped_perceptual\": sorted(list(human_perc)),\n",
    "        \"human_mapped_reasoning\": sorted(list(human_reas)),\n",
    "        \"cat_p\": cat_p,\n",
    "        \"cat_r\": cat_r,\n",
    "        \"best_from_cat_p\": best_from_cat_p,\n",
    "        \"best_from_cat_r\": best_from_cat_r,\n",
    "        \"perceptual_scores\": {name: sc for name, sc in perc_scores},\n",
    "        \"reasoning_scores\": {name: sc for name, sc in reas_scores},\n",
    "    })\n",
    "\n",
    "# --- Save output ---\n",
    "with open('tagged_ALL.json', 'w') as f:\n",
    "    json.dump(tagged_data, f, indent=2)\n",
    "\n",
    "print(\"Skill tagging complete and results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00298cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Perceptual Skill Counts ---\n",
      "Paralinguistic/Emotion Recognition: 1115\n",
      "Speaker Demographics: 735\n",
      "Prosody Detection: 448\n",
      "Speech Activity, Turn-Taking and Overlap Detection: 381\n",
      "Language Identification: 340\n",
      "Speaker Identification: 285\n",
      "Lexical and Phrase-Level Recognition: 230\n",
      "Audio Quality, Artifacts & Channel Characteristics: 58\n",
      "\n",
      "--- Reasoning Skill Counts ---\n",
      "Semantic Abstraction and Summarization: 858\n",
      "Cross-frontier Entity Linking: 556\n",
      "Speaker Intent, Pragmatics and Causal Reasoning: 316\n",
      "Ground Truth and World Knowledge Integration: 299\n",
      "Social Role and Relationship Inference: 200\n",
      "Temporal and Ordering Reasoning: 188\n",
      "Comparative and Preference-Based Judgments: 72\n",
      "Logical/Consistency Reasoning: 65\n",
      "Quantitative Reasoning (Counting/Arithmetic Comparison): 8\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# --- Load the tagged output ---\n",
    "with open('tagged_ALL.json', 'r') as f:\n",
    "    tagged_data = json.load(f)\n",
    "\n",
    "# --- Initialize counters ---\n",
    "perceptual_counter = Counter()\n",
    "reasoning_counter = Counter()\n",
    "\n",
    "# --- Count skill occurrences ---\n",
    "for entry in tagged_data:\n",
    "    perceptual_counter.update(entry.get(\"perceptual_skills\", []))\n",
    "    reasoning_counter.update(entry.get(\"reasoning_skills\", []))\n",
    "\n",
    "# --- Display the counts sorted ---\n",
    "print(\"\\n--- Perceptual Skill Counts ---\")\n",
    "for skill, count in perceptual_counter.most_common():\n",
    "    print(f\"{skill}: {count}\")\n",
    "\n",
    "print(\"\\n--- Reasoning Skill Counts ---\")\n",
    "for skill, count in reasoning_counter.most_common():\n",
    "    print(f\"{skill}: {count}\")\n",
    "\n",
    "# Optionally save to JSON\n",
    "with open('skill_counts.json', 'w') as f:\n",
    "    json.dump({\n",
    "        \"perceptual\": dict(perceptual_counter),\n",
    "        \"reasoning\": dict(reasoning_counter)\n",
    "    }, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "af3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
